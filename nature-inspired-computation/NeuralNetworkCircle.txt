1)
All after 1k epoch - Test Loss / Training Loss
1 hidden layer  - 0.025/0.014
2 hidden layers - 0.022/0.006
5 hidden layers - 0.035/0.008

Neuron testing (above neurons for 2 layers was 4/2)
2/2 - Bad results, no convergence
3/2 - 0.031/0.009
5/2 - 0.024/0.009 (= 4/2)
4/3 - 0.025/0.008 (< 4/2)
4/4 - 0.030/0.009 (> 4/2)

Hidden layers: 2
Minimum epochs for good results: 1k, 2k or beyond doesn't provide significant benefit
Neurons: 4/3 is ideal, but 3/2 is passable. 5/2 | 4/4 or beyond is excessive.

2)
0.030/0.003

Reducing the ratio by 10% (increasing test data) reduced the test loss but increased the training loss. 0.010/0.013

Increasing the ratio by 10% (increasing training data) increased both losses.

3)
Setting noise to 0 gives a test and training loss of 0.
Increasing the noise seems to increase both losses significantly.

There is no overfitting; higher noise leads to underfitting.

4)
Deselecting either feature means the model cannot predict the class at all.
x2 only 0.225/0.242
x1 only 0.271/0.263
x1 x2 x12 0.036/0.008 > x1 x2

x1 x2 x22 0.025/0.005 < x1 x2
x1 x2 x1x2 0.02/0.004 < x1 x2
x1 x2 sin(x1) 0.022/0.004 < x1 x2
x1 x2 sin(x2) 0.016/0.003 < x1 x2
x1 x2 both sins 0.02/0.003

Since x2 alone gives a lower loss, it seems that x2 is slightly more important to the performance of the NN.

5)
0.01 made it take longer to converge to the same value as 0.03
All values before this do the same, but take a lot longer, no performance improvement.
0.1 made it converge very fast, and slightly improved training loss at 0.004.
0.3 led to overfitting, and slightly worsened training loss, oscillating at around 0.01.
Above this makes the oscillation and premature convergence even worse.

6)
ReLU gave slightly better performance
Sigmoid gave worse performance
Linear didn't converge (stayed at 0.5 loss for both)

Tanh (0.021/0.007) and ReLU (0.018/0.006) were quite interchangable, but ReLU is slightly better and therefore the best for this configuration.